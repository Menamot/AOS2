{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d5aeed",
   "metadata": {},
   "source": [
    "# Skipgram model trained on \"20000 lieues sous les mers\"\n",
    "\n",
    "## Needed libraries\n",
    "\n",
    "You will need the following new libraries:\n",
    "- `spacy` for tokenizing\n",
    "- `gensim` for cosine similarities (use `gensim>=4.0.0`)\n",
    "\n",
    "You will also need to download rules for tokenizing a french text.\n",
    "```python\n",
    "python -m spacy download fr_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T19:04:28.854905900Z",
     "start_time": "2024-01-07T19:04:28.820933200Z"
    }
   },
   "id": "8f01914a824cdae3"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "spacy_fr = spacy.load(\"fr_core_news_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T19:04:29.869034300Z",
     "start_time": "2024-01-07T19:04:28.823446300Z"
    }
   },
   "id": "429520904bd5a402"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizing the corpus"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "686e8ef23ff4d3a"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Use a french tokenizer to Create a tokenizer for the french language\n",
    "with open(\"data/20_000_lieues_sous_les_mers.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    document = spacy_fr.tokenizer(f.read())\n",
    "\n",
    "# Define a filtered set of tokens by iterating on `document`. Define a\n",
    "# subset of tokens that are\n",
    "#\n",
    "# - alphanumeric\n",
    "# - in lower case\n",
    "# <answer>\n",
    "tokens = [\n",
    "    tok.text.lower()\n",
    "    for tok in document if tok.is_alpha or tok.is_digit\n",
    "]\n",
    "# </answer>\n",
    "\n",
    "# Make a list of unique tokens and dictionary that maps tokens to\n",
    "# their index in that list.\n",
    "# <answer>\n",
    "idx2tok = list(set(tokens))\n",
    "tok2idx = {token: i for i, token in enumerate(idx2tok)}\n",
    "# </answer>"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T19:04:31.970372Z",
     "start_time": "2024-01-07T19:04:29.870540600Z"
    }
   },
   "id": "6d151ba552e1c374"
  },
  {
   "cell_type": "markdown",
   "id": "471ab7f0",
   "metadata": {},
   "source": [
    "## The continuous bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d34fa0d8",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2024-01-07T19:04:31.990073800Z",
     "start_time": "2024-01-07T19:04:31.970372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Skipgram(\n  (embeddings): Embedding(14709, 32)\n  (U_transpose): Linear(in_features=32, out_features=14709, bias=False)\n)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Define an Embedding module (`nn.Embedding`) and a linear\n",
    "        # transform (`nn.Linear`) without bias.\n",
    "        # <answer>\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.U_transpose = nn.Linear(self.embedding_size, self.vocab_size, bias=False)\n",
    "        # </answer>\n",
    "\n",
    "    def forward(self, center):\n",
    "        # Implements the forward pass of the skipgram model\n",
    "        # `center` is of size `batch_size`\n",
    "\n",
    "        # `e_i` is of size `batch_size` * `embedding_size`\n",
    "        # <answer>\n",
    "        e_i = self.embeddings(center)\n",
    "        # </answer>\n",
    "\n",
    "        # `UT_e_i` is of size `batch_size` * `vocab_size`\n",
    "        # <answer>\n",
    "        UT_e_i = self.U_transpose(e_i)\n",
    "        # </answer>\n",
    "\n",
    "        # <answer>\n",
    "        return UT_e_i\n",
    "        # </answer>\n",
    "\n",
    "\n",
    "# Set the size of vocabulary and size of embedding\n",
    "VOCAB_SIZE = len(idx2tok)\n",
    "EMBEDDING_SIZE = 32\n",
    "\n",
    "# Create a Continuous bag of words model\n",
    "skipgram = Skipgram(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "\n",
    "# Send to GPU if any\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "skipgram.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd2782",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21e39977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T19:04:32.146865900Z",
     "start_time": "2024-01-07T19:04:31.984061200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate n-grams for a given list of tokens, use yield, use window length of n-grams\n",
    "def ngrams_iterator(token_list, ngrams):\n",
    "    \"\"\"Generates successive N-grams from a list of tokens.\"\"\"\n",
    "\n",
    "    for i in range(len(token_list) - ngrams + 1):\n",
    "        idxs = [tok2idx[tok] for tok in token_list[i:i+ngrams]]\n",
    "\n",
    "        # Get center element in `idxs`\n",
    "        center = idxs.pop(ngrams // 2)\n",
    "\n",
    "        # Yield the index of center word and indexes of context words\n",
    "        # as a Numpy array (for Pytorch to automatically convert it to\n",
    "        # a Tensor).\n",
    "        yield center, np.array(idxs)\n",
    "\n",
    "\n",
    "# Create center, context data\n",
    "NGRAMS = 5\n",
    "ngrams = list(ngrams_iterator(tokens, NGRAMS))\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "data = torch.utils.data.DataLoader(ngrams, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee0b37",
   "metadata": {},
   "source": [
    "## Learn Skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfa8ce3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T19:04:32.152560400Z",
     "start_time": "2024-01-07T19:04:32.147869900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the Adam algorithm on the parameters of `skipgram` with a learning\n",
    "# rate of 0.01\n",
    "# <answer>\n",
    "optimizer = optim.Adam(skipgram.parameters(), lr=0.01)\n",
    "# </answer>\n",
    "\n",
    "# Use a cross-entropy loss from the `nn` submodule\n",
    "# <answer>\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8cfd450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T19:08:43.969129100Z",
     "start_time": "2024-01-07T19:08:30.759104100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/20), batch: (0/272), loss: 9.647793769836426\n",
      "Epoch (1/20), batch: (20/272), loss: 9.465937614440918\n",
      "Epoch (1/20), batch: (40/272), loss: 9.260119438171387\n",
      "Epoch (1/20), batch: (60/272), loss: 9.017054557800293\n",
      "Epoch (1/20), batch: (80/272), loss: 8.789603233337402\n",
      "Epoch (1/20), batch: (100/272), loss: 8.609062194824219\n",
      "Epoch (1/20), batch: (120/272), loss: 8.457154273986816\n",
      "Epoch (1/20), batch: (140/272), loss: 8.3346529006958\n",
      "Epoch (1/20), batch: (160/272), loss: 8.22831916809082\n",
      "Epoch (1/20), batch: (180/272), loss: 8.138045310974121\n",
      "Epoch (1/20), batch: (200/272), loss: 8.058736801147461\n",
      "Epoch (1/20), batch: (220/272), loss: 7.991760730743408\n",
      "Epoch (1/20), batch: (240/272), loss: 7.9333295822143555\n",
      "Epoch (1/20), batch: (260/272), loss: 7.8804240226745605\n",
      "1/20 loss 7.85\n",
      "Epoch (2/20), batch: (0/272), loss: 6.820814609527588\n",
      "Epoch (2/20), batch: (20/272), loss: 6.828564167022705\n",
      "Epoch (2/20), batch: (40/272), loss: 6.835676193237305\n",
      "Epoch (2/20), batch: (60/272), loss: 6.839871883392334\n",
      "Epoch (2/20), batch: (80/272), loss: 6.846882343292236\n",
      "Epoch (2/20), batch: (100/272), loss: 6.850337028503418\n",
      "Epoch (2/20), batch: (120/272), loss: 6.855566501617432\n",
      "Epoch (2/20), batch: (140/272), loss: 6.860621452331543\n",
      "Epoch (2/20), batch: (160/272), loss: 6.860086917877197\n",
      "Epoch (2/20), batch: (180/272), loss: 6.861527442932129\n",
      "Epoch (2/20), batch: (200/272), loss: 6.859355449676514\n",
      "Epoch (2/20), batch: (220/272), loss: 6.859398365020752\n",
      "Epoch (2/20), batch: (240/272), loss: 6.860111713409424\n",
      "Epoch (2/20), batch: (260/272), loss: 6.859819412231445\n",
      "2/20 loss 6.86\n",
      "Epoch (3/20), batch: (0/272), loss: 6.40651798248291\n",
      "Epoch (3/20), batch: (20/272), loss: 6.498977184295654\n",
      "Epoch (3/20), batch: (40/272), loss: 6.519896507263184\n",
      "Epoch (3/20), batch: (60/272), loss: 6.5302276611328125\n",
      "Epoch (3/20), batch: (80/272), loss: 6.542937755584717\n",
      "Epoch (3/20), batch: (100/272), loss: 6.548290252685547\n",
      "Epoch (3/20), batch: (120/272), loss: 6.558204650878906\n",
      "Epoch (3/20), batch: (140/272), loss: 6.566814422607422\n",
      "Epoch (3/20), batch: (160/272), loss: 6.574967384338379\n",
      "Epoch (3/20), batch: (180/272), loss: 6.579943656921387\n",
      "Epoch (3/20), batch: (200/272), loss: 6.586508750915527\n",
      "Epoch (3/20), batch: (220/272), loss: 6.590551376342773\n",
      "Epoch (3/20), batch: (240/272), loss: 6.596253395080566\n",
      "Epoch (3/20), batch: (260/272), loss: 6.603320121765137\n",
      "3/20 loss 6.60\n",
      "Epoch (4/20), batch: (0/272), loss: 6.33144998550415\n",
      "Epoch (4/20), batch: (20/272), loss: 6.367016315460205\n",
      "Epoch (4/20), batch: (40/272), loss: 6.371138095855713\n",
      "Epoch (4/20), batch: (60/272), loss: 6.380418300628662\n",
      "Epoch (4/20), batch: (80/272), loss: 6.391517162322998\n",
      "Epoch (4/20), batch: (100/272), loss: 6.4006028175354\n",
      "Epoch (4/20), batch: (120/272), loss: 6.406975746154785\n",
      "Epoch (4/20), batch: (140/272), loss: 6.4156413078308105\n",
      "Epoch (4/20), batch: (160/272), loss: 6.425506114959717\n",
      "Epoch (4/20), batch: (180/272), loss: 6.4323320388793945\n",
      "Epoch (4/20), batch: (200/272), loss: 6.438908100128174\n",
      "Epoch (4/20), batch: (220/272), loss: 6.447962760925293\n",
      "Epoch (4/20), batch: (240/272), loss: 6.454733371734619\n",
      "Epoch (4/20), batch: (260/272), loss: 6.45773458480835\n",
      "4/20 loss 6.46\n",
      "Epoch (5/20), batch: (0/272), loss: 6.281513214111328\n",
      "Epoch (5/20), batch: (20/272), loss: 6.225200176239014\n",
      "Epoch (5/20), batch: (40/272), loss: 6.249599933624268\n",
      "Epoch (5/20), batch: (60/272), loss: 6.263611316680908\n",
      "Epoch (5/20), batch: (80/272), loss: 6.279872417449951\n",
      "Epoch (5/20), batch: (100/272), loss: 6.288357734680176\n",
      "Epoch (5/20), batch: (120/272), loss: 6.299149036407471\n",
      "Epoch (5/20), batch: (140/272), loss: 6.312475204467773\n",
      "Epoch (5/20), batch: (160/272), loss: 6.3195390701293945\n",
      "Epoch (5/20), batch: (180/272), loss: 6.327638149261475\n",
      "Epoch (5/20), batch: (200/272), loss: 6.336266994476318\n",
      "Epoch (5/20), batch: (220/272), loss: 6.344465732574463\n",
      "Epoch (5/20), batch: (240/272), loss: 6.351423263549805\n",
      "Epoch (5/20), batch: (260/272), loss: 6.357690811157227\n",
      "5/20 loss 6.36\n",
      "Epoch (6/20), batch: (0/272), loss: 6.215643405914307\n",
      "Epoch (6/20), batch: (20/272), loss: 6.161607265472412\n",
      "Epoch (6/20), batch: (40/272), loss: 6.180840969085693\n",
      "Epoch (6/20), batch: (60/272), loss: 6.189548969268799\n",
      "Epoch (6/20), batch: (80/272), loss: 6.201201915740967\n",
      "Epoch (6/20), batch: (100/272), loss: 6.212653160095215\n",
      "Epoch (6/20), batch: (120/272), loss: 6.219179630279541\n",
      "Epoch (6/20), batch: (140/272), loss: 6.230451583862305\n",
      "Epoch (6/20), batch: (160/272), loss: 6.237823963165283\n",
      "Epoch (6/20), batch: (180/272), loss: 6.248027324676514\n",
      "Epoch (6/20), batch: (200/272), loss: 6.255629062652588\n",
      "Epoch (6/20), batch: (220/272), loss: 6.2634196281433105\n",
      "Epoch (6/20), batch: (240/272), loss: 6.272254467010498\n",
      "Epoch (6/20), batch: (260/272), loss: 6.280104637145996\n",
      "6/20 loss 6.28\n",
      "Epoch (7/20), batch: (0/272), loss: 6.078340530395508\n",
      "Epoch (7/20), batch: (20/272), loss: 6.092086315155029\n",
      "Epoch (7/20), batch: (40/272), loss: 6.117394924163818\n",
      "Epoch (7/20), batch: (60/272), loss: 6.127999782562256\n",
      "Epoch (7/20), batch: (80/272), loss: 6.142115116119385\n",
      "Epoch (7/20), batch: (100/272), loss: 6.154914855957031\n",
      "Epoch (7/20), batch: (120/272), loss: 6.167405128479004\n",
      "Epoch (7/20), batch: (140/272), loss: 6.176283836364746\n",
      "Epoch (7/20), batch: (160/272), loss: 6.181818008422852\n",
      "Epoch (7/20), batch: (180/272), loss: 6.18760347366333\n",
      "Epoch (7/20), batch: (200/272), loss: 6.195714950561523\n",
      "Epoch (7/20), batch: (220/272), loss: 6.203007221221924\n",
      "Epoch (7/20), batch: (240/272), loss: 6.208059310913086\n",
      "Epoch (7/20), batch: (260/272), loss: 6.214548110961914\n",
      "7/20 loss 6.22\n",
      "Epoch (8/20), batch: (0/272), loss: 6.014892578125\n",
      "Epoch (8/20), batch: (20/272), loss: 6.030460834503174\n",
      "Epoch (8/20), batch: (40/272), loss: 6.058986186981201\n",
      "Epoch (8/20), batch: (60/272), loss: 6.065857410430908\n",
      "Epoch (8/20), batch: (80/272), loss: 6.079456329345703\n",
      "Epoch (8/20), batch: (100/272), loss: 6.093990325927734\n",
      "Epoch (8/20), batch: (120/272), loss: 6.1034626960754395\n",
      "Epoch (8/20), batch: (140/272), loss: 6.108539581298828\n",
      "Epoch (8/20), batch: (160/272), loss: 6.115903377532959\n",
      "Epoch (8/20), batch: (180/272), loss: 6.124788284301758\n",
      "Epoch (8/20), batch: (200/272), loss: 6.134067535400391\n",
      "Epoch (8/20), batch: (220/272), loss: 6.141202449798584\n",
      "Epoch (8/20), batch: (240/272), loss: 6.148198127746582\n",
      "Epoch (8/20), batch: (260/272), loss: 6.155007362365723\n",
      "8/20 loss 6.16\n",
      "Epoch (9/20), batch: (0/272), loss: 6.000828266143799\n",
      "Epoch (9/20), batch: (20/272), loss: 5.9832868576049805\n",
      "Epoch (9/20), batch: (40/272), loss: 5.996574878692627\n",
      "Epoch (9/20), batch: (60/272), loss: 6.011141777038574\n",
      "Epoch (9/20), batch: (80/272), loss: 6.025589466094971\n",
      "Epoch (9/20), batch: (100/272), loss: 6.03841495513916\n",
      "Epoch (9/20), batch: (120/272), loss: 6.050210475921631\n",
      "Epoch (9/20), batch: (140/272), loss: 6.057153701782227\n",
      "Epoch (9/20), batch: (160/272), loss: 6.0664753913879395\n",
      "Epoch (9/20), batch: (180/272), loss: 6.0758867263793945\n",
      "Epoch (9/20), batch: (200/272), loss: 6.083560943603516\n",
      "Epoch (9/20), batch: (220/272), loss: 6.09168004989624\n",
      "Epoch (9/20), batch: (240/272), loss: 6.097148418426514\n",
      "Epoch (9/20), batch: (260/272), loss: 6.104653835296631\n",
      "9/20 loss 6.11\n",
      "Epoch (10/20), batch: (0/272), loss: 5.914487361907959\n",
      "Epoch (10/20), batch: (20/272), loss: 5.936100482940674\n",
      "Epoch (10/20), batch: (40/272), loss: 5.950531005859375\n",
      "Epoch (10/20), batch: (60/272), loss: 5.959613800048828\n",
      "Epoch (10/20), batch: (80/272), loss: 5.978114604949951\n",
      "Epoch (10/20), batch: (100/272), loss: 5.991511821746826\n",
      "Epoch (10/20), batch: (120/272), loss: 6.002224445343018\n",
      "Epoch (10/20), batch: (140/272), loss: 6.009786605834961\n",
      "Epoch (10/20), batch: (160/272), loss: 6.018585205078125\n",
      "Epoch (10/20), batch: (180/272), loss: 6.026556491851807\n",
      "Epoch (10/20), batch: (200/272), loss: 6.035852909088135\n",
      "Epoch (10/20), batch: (220/272), loss: 6.043668746948242\n",
      "Epoch (10/20), batch: (240/272), loss: 6.050347805023193\n",
      "Epoch (10/20), batch: (260/272), loss: 6.057761192321777\n",
      "10/20 loss 6.06\n",
      "Epoch (11/20), batch: (0/272), loss: 5.873923301696777\n",
      "Epoch (11/20), batch: (20/272), loss: 5.890567302703857\n",
      "Epoch (11/20), batch: (40/272), loss: 5.9110565185546875\n",
      "Epoch (11/20), batch: (60/272), loss: 5.925593852996826\n",
      "Epoch (11/20), batch: (80/272), loss: 5.936535358428955\n",
      "Epoch (11/20), batch: (100/272), loss: 5.947816848754883\n",
      "Epoch (11/20), batch: (120/272), loss: 5.962186813354492\n",
      "Epoch (11/20), batch: (140/272), loss: 5.971128463745117\n",
      "Epoch (11/20), batch: (160/272), loss: 5.98131799697876\n",
      "Epoch (11/20), batch: (180/272), loss: 5.989229679107666\n",
      "Epoch (11/20), batch: (200/272), loss: 5.995460033416748\n",
      "Epoch (11/20), batch: (220/272), loss: 6.003555774688721\n",
      "Epoch (11/20), batch: (240/272), loss: 6.009856700897217\n",
      "Epoch (11/20), batch: (260/272), loss: 6.016021251678467\n",
      "11/20 loss 6.02\n",
      "Epoch (12/20), batch: (0/272), loss: 5.780954837799072\n",
      "Epoch (12/20), batch: (20/272), loss: 5.846500396728516\n",
      "Epoch (12/20), batch: (40/272), loss: 5.880746364593506\n",
      "Epoch (12/20), batch: (60/272), loss: 5.899168968200684\n",
      "Epoch (12/20), batch: (80/272), loss: 5.905591011047363\n",
      "Epoch (12/20), batch: (100/272), loss: 5.917144298553467\n",
      "Epoch (12/20), batch: (120/272), loss: 5.92326545715332\n",
      "Epoch (12/20), batch: (140/272), loss: 5.930062770843506\n",
      "Epoch (12/20), batch: (160/272), loss: 5.940227508544922\n",
      "Epoch (12/20), batch: (180/272), loss: 5.946039199829102\n",
      "Epoch (12/20), batch: (200/272), loss: 5.954889297485352\n",
      "Epoch (12/20), batch: (220/272), loss: 5.963267803192139\n",
      "Epoch (12/20), batch: (240/272), loss: 5.968735694885254\n",
      "Epoch (12/20), batch: (260/272), loss: 5.976243019104004\n",
      "12/20 loss 5.98\n",
      "Epoch (13/20), batch: (0/272), loss: 5.802495956420898\n",
      "Epoch (13/20), batch: (20/272), loss: 5.832528591156006\n",
      "Epoch (13/20), batch: (40/272), loss: 5.850804328918457\n",
      "Epoch (13/20), batch: (60/272), loss: 5.862401962280273\n",
      "Epoch (13/20), batch: (80/272), loss: 5.8740339279174805\n",
      "Epoch (13/20), batch: (100/272), loss: 5.881953239440918\n",
      "Epoch (13/20), batch: (120/272), loss: 5.89538049697876\n",
      "Epoch (13/20), batch: (140/272), loss: 5.9031853675842285\n",
      "Epoch (13/20), batch: (160/272), loss: 5.909668445587158\n",
      "Epoch (13/20), batch: (180/272), loss: 5.916308403015137\n",
      "Epoch (13/20), batch: (200/272), loss: 5.925609111785889\n",
      "Epoch (13/20), batch: (220/272), loss: 5.930750846862793\n",
      "Epoch (13/20), batch: (240/272), loss: 5.935379981994629\n",
      "Epoch (13/20), batch: (260/272), loss: 5.943240165710449\n",
      "13/20 loss 5.95\n",
      "Epoch (14/20), batch: (0/272), loss: 5.703157424926758\n",
      "Epoch (14/20), batch: (20/272), loss: 5.800982475280762\n",
      "Epoch (14/20), batch: (40/272), loss: 5.795451641082764\n",
      "Epoch (14/20), batch: (60/272), loss: 5.821157455444336\n",
      "Epoch (14/20), batch: (80/272), loss: 5.832963943481445\n",
      "Epoch (14/20), batch: (100/272), loss: 5.842650890350342\n",
      "Epoch (14/20), batch: (120/272), loss: 5.854220867156982\n",
      "Epoch (14/20), batch: (140/272), loss: 5.862687587738037\n",
      "Epoch (14/20), batch: (160/272), loss: 5.874438762664795\n",
      "Epoch (14/20), batch: (180/272), loss: 5.880987644195557\n",
      "Epoch (14/20), batch: (200/272), loss: 5.888557434082031\n",
      "Epoch (14/20), batch: (220/272), loss: 5.896726131439209\n",
      "Epoch (14/20), batch: (240/272), loss: 5.906238079071045\n",
      "Epoch (14/20), batch: (260/272), loss: 5.913636684417725\n",
      "14/20 loss 5.92\n",
      "Epoch (15/20), batch: (0/272), loss: 5.793935298919678\n",
      "Epoch (15/20), batch: (20/272), loss: 5.792266368865967\n",
      "Epoch (15/20), batch: (40/272), loss: 5.785956382751465\n",
      "Epoch (15/20), batch: (60/272), loss: 5.801545143127441\n",
      "Epoch (15/20), batch: (80/272), loss: 5.810163974761963\n",
      "Epoch (15/20), batch: (100/272), loss: 5.821126461029053\n",
      "Epoch (15/20), batch: (120/272), loss: 5.8326005935668945\n",
      "Epoch (15/20), batch: (140/272), loss: 5.8431901931762695\n",
      "Epoch (15/20), batch: (160/272), loss: 5.852504730224609\n",
      "Epoch (15/20), batch: (180/272), loss: 5.86245584487915\n",
      "Epoch (15/20), batch: (200/272), loss: 5.868624687194824\n",
      "Epoch (15/20), batch: (220/272), loss: 5.873115062713623\n",
      "Epoch (15/20), batch: (240/272), loss: 5.88096284866333\n",
      "Epoch (15/20), batch: (260/272), loss: 5.885436534881592\n",
      "15/20 loss 5.89\n",
      "Epoch (16/20), batch: (0/272), loss: 5.671010971069336\n",
      "Epoch (16/20), batch: (20/272), loss: 5.7489237785339355\n",
      "Epoch (16/20), batch: (40/272), loss: 5.758645057678223\n",
      "Epoch (16/20), batch: (60/272), loss: 5.772246837615967\n",
      "Epoch (16/20), batch: (80/272), loss: 5.786615371704102\n",
      "Epoch (16/20), batch: (100/272), loss: 5.795528411865234\n",
      "Epoch (16/20), batch: (120/272), loss: 5.802656650543213\n",
      "Epoch (16/20), batch: (140/272), loss: 5.811100959777832\n",
      "Epoch (16/20), batch: (160/272), loss: 5.822205066680908\n",
      "Epoch (16/20), batch: (180/272), loss: 5.832369327545166\n",
      "Epoch (16/20), batch: (200/272), loss: 5.838961124420166\n",
      "Epoch (16/20), batch: (220/272), loss: 5.846112251281738\n",
      "Epoch (16/20), batch: (240/272), loss: 5.854966640472412\n",
      "Epoch (16/20), batch: (260/272), loss: 5.860358715057373\n",
      "16/20 loss 5.86\n",
      "Epoch (17/20), batch: (0/272), loss: 5.785734176635742\n",
      "Epoch (17/20), batch: (20/272), loss: 5.736727714538574\n",
      "Epoch (17/20), batch: (40/272), loss: 5.740757942199707\n",
      "Epoch (17/20), batch: (60/272), loss: 5.758941173553467\n",
      "Epoch (17/20), batch: (80/272), loss: 5.771304130554199\n",
      "Epoch (17/20), batch: (100/272), loss: 5.776731967926025\n",
      "Epoch (17/20), batch: (120/272), loss: 5.784578323364258\n",
      "Epoch (17/20), batch: (140/272), loss: 5.795685291290283\n",
      "Epoch (17/20), batch: (160/272), loss: 5.802687644958496\n",
      "Epoch (17/20), batch: (180/272), loss: 5.811578750610352\n",
      "Epoch (17/20), batch: (200/272), loss: 5.816531181335449\n",
      "Epoch (17/20), batch: (220/272), loss: 5.8241047859191895\n",
      "Epoch (17/20), batch: (240/272), loss: 5.83204984664917\n",
      "Epoch (17/20), batch: (260/272), loss: 5.838003635406494\n",
      "17/20 loss 5.84\n",
      "Epoch (18/20), batch: (0/272), loss: 5.724590301513672\n",
      "Epoch (18/20), batch: (20/272), loss: 5.694728851318359\n",
      "Epoch (18/20), batch: (40/272), loss: 5.707675457000732\n",
      "Epoch (18/20), batch: (60/272), loss: 5.722259044647217\n",
      "Epoch (18/20), batch: (80/272), loss: 5.732600212097168\n",
      "Epoch (18/20), batch: (100/272), loss: 5.751614093780518\n",
      "Epoch (18/20), batch: (120/272), loss: 5.762081623077393\n",
      "Epoch (18/20), batch: (140/272), loss: 5.7719268798828125\n",
      "Epoch (18/20), batch: (160/272), loss: 5.7828288078308105\n",
      "Epoch (18/20), batch: (180/272), loss: 5.788605690002441\n",
      "Epoch (18/20), batch: (200/272), loss: 5.796384811401367\n",
      "Epoch (18/20), batch: (220/272), loss: 5.802145481109619\n",
      "Epoch (18/20), batch: (240/272), loss: 5.80897331237793\n",
      "Epoch (18/20), batch: (260/272), loss: 5.8171491622924805\n",
      "18/20 loss 5.82\n",
      "Epoch (19/20), batch: (0/272), loss: 5.727845191955566\n",
      "Epoch (19/20), batch: (20/272), loss: 5.660831928253174\n",
      "Epoch (19/20), batch: (40/272), loss: 5.702462196350098\n",
      "Epoch (19/20), batch: (60/272), loss: 5.709485054016113\n",
      "Epoch (19/20), batch: (80/272), loss: 5.725092887878418\n",
      "Epoch (19/20), batch: (100/272), loss: 5.736981391906738\n",
      "Epoch (19/20), batch: (120/272), loss: 5.743753910064697\n",
      "Epoch (19/20), batch: (140/272), loss: 5.750784397125244\n",
      "Epoch (19/20), batch: (160/272), loss: 5.759171962738037\n",
      "Epoch (19/20), batch: (180/272), loss: 5.767590522766113\n",
      "Epoch (19/20), batch: (200/272), loss: 5.777270793914795\n",
      "Epoch (19/20), batch: (220/272), loss: 5.784595966339111\n",
      "Epoch (19/20), batch: (240/272), loss: 5.792102813720703\n",
      "Epoch (19/20), batch: (260/272), loss: 5.800907611846924\n",
      "19/20 loss 5.80\n",
      "Epoch (20/20), batch: (0/272), loss: 5.751400947570801\n",
      "Epoch (20/20), batch: (20/272), loss: 5.696022033691406\n",
      "Epoch (20/20), batch: (40/272), loss: 5.689519882202148\n",
      "Epoch (20/20), batch: (60/272), loss: 5.705704689025879\n",
      "Epoch (20/20), batch: (80/272), loss: 5.7134175300598145\n",
      "Epoch (20/20), batch: (100/272), loss: 5.725538730621338\n",
      "Epoch (20/20), batch: (120/272), loss: 5.735661029815674\n",
      "Epoch (20/20), batch: (140/272), loss: 5.746320724487305\n",
      "Epoch (20/20), batch: (160/272), loss: 5.752143383026123\n",
      "Epoch (20/20), batch: (180/272), loss: 5.759304046630859\n",
      "Epoch (20/20), batch: (200/272), loss: 5.767123699188232\n",
      "Epoch (20/20), batch: (220/272), loss: 5.773360252380371\n",
      "Epoch (20/20), batch: (240/272), loss: 5.778628349304199\n",
      "Epoch (20/20), batch: (260/272), loss: 5.784717082977295\n",
      "20/20 loss 5.79\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0\n",
    "    for i, (center, context) in enumerate(data):\n",
    "        center, context = center.to(device), context.to(device)\n",
    "        context = context.to(torch.int64)\n",
    "        # Reset the gradients of the computational graph\n",
    "        # <answer>\n",
    "        skipgram.zero_grad()\n",
    "        # </answer>\n",
    "\n",
    "        # Forward pass\n",
    "        # <answer>\n",
    "        UT_e_i = skipgram.forward(center)\n",
    "        # </answer>\n",
    "\n",
    "        # Define one-hot encoding for tokens in context. `one_hots` has the same\n",
    "        # size as `UT_e_i` and is zero everywhere except at location\n",
    "        # corresponding to `context`. You can use `torch.scatter`.\n",
    "        # <answer>\n",
    "        one_hots = torch.zeros_like(UT_e_i).scatter(1, context, 1/(NGRAMS-1))\n",
    "        # </answer>\n",
    "\n",
    "        # Compute loss between `UT_e_i` and `one_hots`\n",
    "        # <answer>\n",
    "        loss = ce_loss(UT_e_i, one_hots)\n",
    "        # </answer>\n",
    "\n",
    "        # Backward pass to compute gradients of each parameter\n",
    "        # <answer>\n",
    "        loss.backward()\n",
    "        # </answer>\n",
    "\n",
    "        # Gradient descent step according to the chosen optimizer\n",
    "        # <answer>\n",
    "        optimizer.step()\n",
    "        # </answer>\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            loss_avg = float(total_loss / (i + 1))\n",
    "            print(\n",
    "                f\"Epoch ({epoch}/{EPOCHS}), batch: ({i}/{len(data)}), loss: {loss_avg}\"\n",
    "            )\n",
    "\n",
    "    # Print average loss after each epoch\n",
    "    loss_avg = float(total_loss / len(data))\n",
    "    print(\"{}/{} loss {:.2f}\".format(epoch, EPOCHS, loss_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f99c6",
   "metadata": {},
   "source": [
    "## Prediction functions\n",
    "\n",
    "Now that the skipgram model is learned we can give it a word and see what\n",
    "context the model predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a5905fd",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2024-01-07T19:09:32.767800900Z",
     "start_time": "2024-01-07T19:09:32.756272200Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_context_words(skipgram, center_word, k=4):\n",
    "    \"\"\"Predicts `k` best context words of `center_word` according to model `skipgram`\"\"\"\n",
    "\n",
    "    # Get index of `center_word`\n",
    "    center_word_idx = tok2idx[center_word]\n",
    "\n",
    "    # Create a fake minibatch containing just `center_word_idx`. Make sure that\n",
    "    # `fake_minibatch` is a Long tensor and don't forget to send it to device.\n",
    "    # <answer>\n",
    "    fake_minibatch = torch.LongTensor([center_word_idx]).unsqueeze(0).to(device)\n",
    "    # </answer>\n",
    "\n",
    "    # Forward propagate through the skipgram model\n",
    "    # <answer>\n",
    "    score_context = skipgram(fake_minibatch).squeeze()\n",
    "    # </answer>\n",
    "\n",
    "    # Retrieve top k-best indexes using `torch.topk`\n",
    "    # <answer>\n",
    "    _, best_idxs = torch.topk(score_context, k=k)\n",
    "    # </answer>\n",
    "\n",
    "    # Return actual tokens using `idx2tok`\n",
    "    # <answer>\n",
    "    return [idx2tok[idx] for idx in best_idxs]\n",
    "    # </answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b9c3dfd",
   "metadata": {
    "lines_to_next_cell": 2,
    "ExecuteTime": {
     "end_time": "2024-01-07T19:17:10.825475200Z",
     "start_time": "2024-01-07T19:17:10.817925300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['de', 'cent', 'deux', 'mètres']"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_context_words(skipgram, \"mille\")\n",
    "# predict_context_words(skipgram, \"nemo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad6c91",
   "metadata": {},
   "source": [
    "## Testing the embedding\n",
    "\n",
    "We use the library `gensim` to easily compute most similar words for\n",
    "the embedding we just learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97621040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T19:12:50.228966800Z",
     "start_time": "2024-01-07T19:12:50.205939Z"
    }
   },
   "outputs": [],
   "source": [
    "m = KeyedVectors(vector_size=EMBEDDING_SIZE)\n",
    "m.add_vectors(idx2tok, skipgram.embeddings.weight.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2290bc",
   "metadata": {},
   "source": [
    "You can now test most similar words for, for example \"lieues\",\n",
    "\"mers\", \"professeur\"... You can look at `words_decreasing_freq` to\n",
    "test most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bf85625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T19:12:51.636123Z",
     "start_time": "2024-01-07T19:12:51.576247700Z"
    }
   },
   "outputs": [],
   "source": [
    "unique, freq = np.unique(tokens, return_counts=True)\n",
    "idxs = freq.argsort()[::-1]\n",
    "words_decreasing_freq = list(zip(unique[idxs], freq[idxs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2742972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T19:17:37.151684400Z",
     "start_time": "2024-01-07T19:17:37.140621100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('pieds', 0.7717189788818359),\n ('kilomètres', 0.764899730682373),\n ('centimètres', 0.7269245386123657),\n ('quarante', 0.7160674929618835),\n ('milles', 0.7121396660804749),\n ('globes', 0.7067031860351562),\n ('télégrammes', 0.697126567363739),\n ('cent', 0.6750165820121765),\n ('kilogrammes', 0.6702409982681274),\n ('septième', 0.6674327850341797)]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <answer>\n",
    "m.most_similar(\"lieues\")\n",
    "m.most_similar(\"professeur\")\n",
    "m.most_similar(\"mers\")\n",
    "m.most_similar(\"noire\")\n",
    "m.most_similar(\"mètres\")\n",
    "m.most_similar(\"ma\")\n",
    "# </answer>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
