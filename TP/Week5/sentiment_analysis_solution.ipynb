{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094fa655",
   "metadata": {},
   "source": [
    "# Word embedding and RNN for sentiment analysis\n",
    "\n",
    "The goal of the following notebook is to predict whether a written\n",
    "critic about a movie is positive or negative. For that we will try\n",
    "three models. A simple linear model on the word embeddings, a\n",
    "recurrent neural network and a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd09a88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T22:00:24.181168200Z",
     "start_time": "2024-01-09T22:00:21.398787100Z"
    }
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "import appdirs                  # Used to cache pretrained embeddings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98d925",
   "metadata": {},
   "source": [
    "## The IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ab1832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T22:00:24.999226400Z",
     "start_time": "2024-01-09T22:00:24.182168300Z"
    }
   },
   "outputs": [],
   "source": [
    "torch_cache = appdirs.user_cache_dir(\"pytorch\")\n",
    "train_iter, test_iter = IMDB(root=torch_cache, split=(\"train\", \"test\"))\n",
    "\n",
    "import random\n",
    "\n",
    "TRAIN_SET = list(train_iter)\n",
    "TEST_SET = list(test_iter)\n",
    "random.shuffle(TRAIN_SET)\n",
    "random.shuffle(TEST_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "522a5228",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T22:00:25.008484100Z",
     "start_time": "2024-01-09T22:00:25.001209200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(2,\n \"I recorded this ages ago but only got round to watching it today. I have been ill so had run out of stuff to watch! I am so glad I saw it, and which I could erase my memory and watch i again for the first time. This movie is so wonderful! It reminded me very much of Fried Green Tomatoes At The Whistlestop Cafe. <br /><br />The story goes back in time and at the end of the movie we see what the connections are. Some people have said this is a kids movie. I disagree - it may be made by Disney and many characters are children, but I am 23 and I LOVED it! There were moments when my spine tingled. The story is unlike any other film these days, full of adventure. I have just ordered the book from amazon, can't wait!\")"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SET[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bd9be",
   "metadata": {},
   "source": [
    "## Global variables\n",
    "\n",
    "First let's define a few variables. `EMBEDDING_DIM` is the dimension\n",
    "of the vector space used to embed all the words of the vocabulary.\n",
    "`SEQ_LENGTH` is the maximum length of a sequence, `BATCH_SIZE` is\n",
    "the size of the batches used in stochastic optimization algorithms\n",
    "and `NUM_EPOCHS` the number of times we are going thought the entire\n",
    "training set during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d571ecc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T22:00:25.023458300Z",
     "start_time": "2024-01-09T22:00:25.006474200Z"
    }
   },
   "outputs": [],
   "source": [
    "# <answer>\n",
    "EMBEDDING_DIM = 8\n",
    "SEQ_LENGTH = 64\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 10\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907d78d",
   "metadata": {},
   "source": [
    "We first need a tokenizer that take a text a returns a list of\n",
    "tokens. There are many tokenizers available from other libraries.\n",
    "Here we use the one that comes with Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a57107",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2024-01-09T22:00:25.044410700Z",
     "start_time": "2024-01-09T22:00:25.021951600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['all', 'your', 'base', 'are', 'belong', 'to', 'us']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokenizer(\"All your base are belong to us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd03d1",
   "metadata": {},
   "source": [
    "## Building the vocabulary\n",
    "\n",
    "Then we need to define the set of words that will be understood by\n",
    "the model: this is the vocabulary. We build it from the training\n",
    "set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bad8f472",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T22:00:27.287013100Z",
     "start_time": "2024-01-09T22:00:25.029391600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "935"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def yield_tokens(data_iter: Iterable) -> List[str]:\n",
    "    for data_sample in data_iter:\n",
    "        yield tokenizer(data_sample[1])\n",
    "\n",
    "\n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(TRAIN_SET),\n",
    "    min_freq=10,\n",
    "    specials=special_tokens,\n",
    "    special_first=True)\n",
    "UNK_IDX, PAD_IDX = vocab.lookup_indices(special_tokens)\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "vocab['plenty']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d3890",
   "metadata": {},
   "source": [
    "\n",
    "To limit the number of tokens in the vocabulary, we specified\n",
    "`min_freq=10`: a token should be seen at least 10 times to be part\n",
    "of the vocabulary. Consequently some words in the training set (and\n",
    "in the test set) are not present in the vocabulary. We then need to\n",
    "set a default index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4a2a74",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2024-01-09T22:00:27.293840200Z",
     "start_time": "2024-01-09T22:00:27.288012700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 935]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab['pouet']                  # Error\n",
    "vocab.set_default_index(UNK_IDX)\n",
    "vocab(['pouet','plenty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44e13f",
   "metadata": {},
   "source": [
    "# Collate function\n",
    "\n",
    "The collate function maps raw samples coming from the dataset to\n",
    "padded tensors of numericalized tokens ready to be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7db28dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T22:00:27.309429900Z",
     "start_time": "2024-01-09T22:00:27.291840600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch: List):\n",
    "    def collate(text):\n",
    "        \"\"\"Turn a text into a tensor of integers.\"\"\"\n",
    "\n",
    "        tokens = tokenizer(text)[:SEQ_LENGTH]\n",
    "        return torch.LongTensor(vocab(tokens))\n",
    "\n",
    "    src_batch = [collate(text) for _, text in batch]\n",
    "\n",
    "    # Pad list of tensors using `pad_sequence`\n",
    "    # <answer>\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    # </answer>\n",
    "\n",
    "    # Turn 2 (positive review) and 1 (negative review) labels into 1 and 0\n",
    "    # <answer>\n",
    "    tgt_batch = torch.Tensor([label - 1 for label, _ in batch])\n",
    "    # </answer>\n",
    "\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "\n",
    "print(f\"Number of training examples: {len(TRAIN_SET)}\")\n",
    "print(f\"Number of testing examples: {len(TEST_SET)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39e88720",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T22:00:27.309429900Z",
     "start_time": "2024-01-09T22:00:27.300382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 13],\n         [246],\n         [  0]]),\n tensor([0.]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_fn([\n",
    "    (1, \"i am Groot\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a6fcf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Training a linear classifier with an embedding\n",
    "\n",
    "We first test a simple linear classifier on the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ef7c91c",
   "metadata": {
    "lines_to_next_cell": 2,
    "ExecuteTime": {
     "end_time": "2024-01-09T22:00:27.353496600Z",
     "start_time": "2024-01-09T22:00:27.309429900Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, seq_length):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Define an embedding of `vocab_size` words into a vector space\n",
    "        # of dimension `embedding_dim`.\n",
    "        # <answer>\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        # </answer>\n",
    "\n",
    "        # Define a linear layer from dimension `seq_length` *\n",
    "        # `embedding_dim` to 1.\n",
    "        # <answer>\n",
    "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
    "        # </answer>\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is of size `seq_length` * `batch_size`\n",
    "\n",
    "        # Compute the embedding `embedded` of the batch `x`. `embedded` is\n",
    "        # of size `batch_size` * `seq_length` * `embedding_dim`\n",
    "        # <answer>\n",
    "        embedded = self.embedding(x)\n",
    "        # </answer>\n",
    "\n",
    "        # Flatten the embedded words and feed it to the linear layer.\n",
    "        # `flatten` is of size `batch_size` * (`seq_length` * `embedding_dim`)\n",
    "        # <answer>\n",
    "        flatten = embedded.view(-1, self.seq_length * self.embedding_dim)\n",
    "        # </answer>\n",
    "\n",
    "        # Apply the linear layer and return a squeezed version\n",
    "        # `l1` is of size `batch_size`\n",
    "        # <answer>\n",
    "        return self.l1(flatten).squeeze()\n",
    "        # </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e8282",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We need to implement an accuracy function to be used in the `Trainer`\n",
    "class (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b8b4cf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T22:00:27.362499900Z",
     "start_time": "2024-01-09T22:00:27.313603700Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    # `predictions` and `labels` are both tensors of same length\n",
    "\n",
    "    # Implement accuracy\n",
    "    # <answer>\n",
    "    return torch.sum((torch.sigmoid(predictions) > 0.5).float() == (labels > .5)).item() / len(\n",
    "        predictions\n",
    "    )\n",
    "    # </answer>\n",
    "\n",
    "\n",
    "assert accuracy(torch.Tensor([1, -2, 3]), torch.Tensor([1, 0, 1])) == 1\n",
    "assert accuracy(torch.Tensor([1, -2, -3]), torch.Tensor([1, 0, 1])) == 2 / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb02a80",
   "metadata": {},
   "source": [
    "Train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d43387df",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2024-01-09T22:00:27.363500600Z",
     "start_time": "2024-01-09T22:00:27.324337500Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, optimizer: Optimizer):\n",
    "    model.to(device)\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        TRAIN_SET, batch_size=BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    matches = 0\n",
    "    losses = 0\n",
    "    for sequences, labels in train_dataloader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        # Implement a step of the algorithm:\n",
    "        #\n",
    "        # - set gradients to zero\n",
    "        # - forward propagate examples in `batch`\n",
    "        # - compute `loss` with chosen criterion\n",
    "        # - back-propagate gradients\n",
    "        # - gradient step\n",
    "        # <answer>\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(sequences)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        # </answer>\n",
    "\n",
    "        acc = accuracy(predictions, labels)\n",
    "\n",
    "        matches += len(predictions) * acc\n",
    "\n",
    "    return losses / len(TRAIN_SET), matches / len(TRAIN_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c84b4b6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T22:07:17.805137300Z",
     "start_time": "2024-01-09T22:07:17.797610100Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        TEST_SET, batch_size=BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    losses = 0\n",
    "    matches = 0\n",
    "    for sequences, labels in val_dataloader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        predictions = model.forward(sequences)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        acc = accuracy(predictions, labels)\n",
    "        matches += len(predictions) * acc\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(TEST_SET), matches / len(TEST_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "496136a3",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2024-01-09T22:07:18.111050800Z",
     "start_time": "2024-01-09T22:07:18.109532200Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer):\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = timer()\n",
    "        train_loss, train_acc = train_epoch(model, optimizer)\n",
    "        end_time = timer()\n",
    "        val_loss, val_acc = evaluate(model)\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, \"\n",
    "            f\"Train loss: {train_loss:.3f}, \"\n",
    "            f\"Train acc: {train_acc:.3f}, \"\n",
    "            f\"Val loss: {val_loss:.3f}, \"\n",
    "            f\"Val acc: {val_acc:.3f}, \"\n",
    "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0723838a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T22:07:18.350013800Z",
     "start_time": "2024-01-09T22:07:18.347485200Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    \"Predict sentiment of given sentence according to model\"\n",
    "\n",
    "    tensor, _ = collate_fn([(\"dummy\", sentence)])\n",
    "    prediction = model(tensor)\n",
    "    pred = torch.sigmoid(prediction)\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65e07e5f",
   "metadata": {
    "lines_to_next_cell": 2,
    "ExecuteTime": {
     "end_time": "2024-01-09T22:07:32.940919Z",
     "start_time": "2024-01-09T22:07:18.507024500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164009\n",
      "Epoch: 1, Train loss: 0.001, Train acc: 0.505, Val loss: 0.001, Val acc: 0.502, Epoch time = 2.366s\n",
      "Epoch: 2, Train loss: 0.001, Train acc: 0.523, Val loss: 0.001, Val acc: 0.503, Epoch time = 2.235s\n",
      "Epoch: 3, Train loss: 0.001, Train acc: 0.538, Val loss: 0.001, Val acc: 0.504, Epoch time = 2.218s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      6\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m Adam(embedding_net\u001B[38;5;241m.\u001B[39mparameters())\n\u001B[1;32m----> 7\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_net\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# # %% [markdown]\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# # ## Training a linear classifier with a pretrained embedding\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# #\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# # Load a GloVe pretrained embedding instead\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Download GloVe word embedding\u001B[39;00m\n\u001B[0;32m     16\u001B[0m glove \u001B[38;5;241m=\u001B[39m torchtext\u001B[38;5;241m.\u001B[39mvocab\u001B[38;5;241m.\u001B[39mGloVe(name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m6B\u001B[39m\u001B[38;5;124m\"\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m100\u001B[39m\u001B[38;5;124m\"\u001B[39m, cache\u001B[38;5;241m=\u001B[39mtorch_cache)\n",
      "Cell \u001B[1;32mIn[20], line 4\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, optimizer)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, NUM_EPOCHS \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m      3\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[1;32m----> 4\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[0;32m      6\u001B[0m     val_loss, val_acc \u001B[38;5;241m=\u001B[39m evaluate(model)\n",
      "Cell \u001B[1;32mIn[13], line 15\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, optimizer)\u001B[0m\n\u001B[0;32m     13\u001B[0m matches \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     14\u001B[0m losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 15\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43msequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43msequences\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Implement a step of the algorithm:\u001B[39;49;00m\n\u001B[0;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#\u001B[39;49;00m\n\u001B[0;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# - set gradients to zero\u001B[39;49;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# - gradient step\u001B[39;49;00m\n\u001B[0;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# <answer>\u001B[39;49;00m\n",
      "File \u001B[1;32mD:\\Programming\\Conda_envs\\ML\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\Programming\\Conda_envs\\ML\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\Programming\\Conda_envs\\ML\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[9], line 8\u001B[0m, in \u001B[0;36mcollate_fn\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m      5\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m tokenizer(text)[:SEQ_LENGTH]\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mLongTensor(vocab(tokens))\n\u001B[1;32m----> 8\u001B[0m src_batch \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Pad list of tensors using `pad_sequence`\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# <answer>\u001B[39;00m\n\u001B[0;32m     12\u001B[0m src_batch \u001B[38;5;241m=\u001B[39m pad_sequence(src_batch, padding_value\u001B[38;5;241m=\u001B[39mPAD_IDX)\n",
      "Cell \u001B[1;32mIn[9], line 8\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      5\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m tokenizer(text)[:SEQ_LENGTH]\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mLongTensor(vocab(tokens))\n\u001B[1;32m----> 8\u001B[0m src_batch \u001B[38;5;241m=\u001B[39m [\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m _, text \u001B[38;5;129;01min\u001B[39;00m batch]\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Pad list of tensors using `pad_sequence`\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# <answer>\u001B[39;00m\n\u001B[0;32m     12\u001B[0m src_batch \u001B[38;5;241m=\u001B[39m pad_sequence(src_batch, padding_value\u001B[38;5;241m=\u001B[39mPAD_IDX)\n",
      "Cell \u001B[1;32mIn[9], line 6\u001B[0m, in \u001B[0;36mcollate_fn.<locals>.collate\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Turn a text into a tensor of integers.\"\"\"\u001B[39;00m\n\u001B[0;32m      5\u001B[0m tokens \u001B[38;5;241m=\u001B[39m tokenizer(text)[:SEQ_LENGTH]\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLongTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "embedding_net = EmbeddingNet(VOCAB_SIZE, EMBEDDING_DIM, SEQ_LENGTH)\n",
    "print(sum(torch.numel(e) for e in embedding_net.parameters()))\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "optimizer = Adam(embedding_net.parameters())\n",
    "train(embedding_net, optimizer)\n",
    "\n",
    "\n",
    "# # %% [markdown]\n",
    "# # ## Training a linear classifier with a pretrained embedding\n",
    "# #\n",
    "# # Load a GloVe pretrained embedding instead\n",
    "\n",
    "# Download GloVe word embedding\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=\"100\", cache=torch_cache)\n",
    "\n",
    "# Get token embedding of our `vocab`\n",
    "vocab_vectors = glove.get_vecs_by_tokens(vocab.get_itos())\n",
    "\n",
    "# tot_transferred = 0\n",
    "# for v in vocab_vectors:\n",
    "#     if not v.equal(torch.zeros(100)):\n",
    "#         tot_transferred += 1\n",
    "\n",
    "# tot_transferred, len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1579f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T22:07:32.941917900Z",
     "start_time": "2024-01-09T22:07:32.941917900Z"
    }
   },
   "outputs": [],
   "source": [
    "class GloVeEmbeddingNet(nn.Module):\n",
    "    def __init__(self, seq_length, vocab_vectors, freeze=True):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Define `embedding_dim` from vocabulary and the pretrained `embedding`.\n",
    "        # <answer>\n",
    "        self.embedding_dim = vocab_vectors.size(1)\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
    "        # </answer>\n",
    "\n",
    "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is of size batch_size * seq_length\n",
    "\n",
    "        # `embedded` is of size batch_size * seq_length * embedding_dim\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # `flatten` is of size batch_size * (seq_length * embedding_dim)\n",
    "        flatten = embedded.view(-1, self.seq_length * self.embedding_dim)\n",
    "\n",
    "        # L1 is of size batch_size\n",
    "        return self.l1(flatten).squeeze()\n",
    "\n",
    "\n",
    "glove_embedding_net1 = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=True)\n",
    "print(sum(torch.numel(e) for e in glove_embedding_net1.parameters()))\n",
    "\n",
    "optimizer = Adam(glove_embedding_net1.parameters())\n",
    "train(glove_embedding_net1, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99b8f7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Use pretrained embedding without fine-tuning\n",
    "\n",
    "Define model and freeze the embedding\n",
    "<answer>\n",
    "glove_embedding_net1 = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=True)\n",
    "</answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6264780",
   "metadata": {},
   "source": [
    "## Fine-tuning the pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8160c662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:45:36.466840900Z",
     "start_time": "2024-01-07T22:45:36.456330100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define model and don't freeze embedding weights\n",
    "# <answer>\n",
    "glove_embedding_net2 = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=False)\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4012315",
   "metadata": {},
   "source": [
    "## Recurrent neural network with frozen pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9a138ba",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2024-01-07T22:49:07.353440600Z",
     "start_time": "2024-01-07T22:45:37.784165800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60701\n",
      "Epoch: 1, Train loss: 0.001, Train acc: 0.564, Val loss: 0.001, Val acc: 0.656, Epoch time = 10.818s\n",
      "Epoch: 2, Train loss: 0.001, Train acc: 0.694, Val loss: 0.001, Val acc: 0.703, Epoch time = 10.348s\n",
      "Epoch: 3, Train loss: 0.001, Train acc: 0.728, Val loss: 0.001, Val acc: 0.729, Epoch time = 10.434s\n",
      "Epoch: 4, Train loss: 0.001, Train acc: 0.746, Val loss: 0.001, Val acc: 0.744, Epoch time = 10.400s\n",
      "Epoch: 5, Train loss: 0.001, Train acc: 0.759, Val loss: 0.001, Val acc: 0.754, Epoch time = 10.488s\n",
      "Epoch: 6, Train loss: 0.001, Train acc: 0.767, Val loss: 0.001, Val acc: 0.762, Epoch time = 10.450s\n",
      "Epoch: 7, Train loss: 0.001, Train acc: 0.773, Val loss: 0.001, Val acc: 0.767, Epoch time = 14.268s\n",
      "Epoch: 8, Train loss: 0.001, Train acc: 0.779, Val loss: 0.001, Val acc: 0.770, Epoch time = 17.793s\n",
      "Epoch: 9, Train loss: 0.001, Train acc: 0.784, Val loss: 0.001, Val acc: 0.772, Epoch time = 17.639s\n",
      "Epoch: 10, Train loss: 0.001, Train acc: 0.790, Val loss: 0.001, Val acc: 0.773, Epoch time = 17.959s\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_vectors, freeze=True):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Define pretrained embedding\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
    "\n",
    "        # Size of input `x_t` from `embedding`\n",
    "        self.embedding_size = self.embedding.embedding_dim\n",
    "        self.input_size = self.embedding_size\n",
    "\n",
    "        # Size of hidden state `h_t`\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define a GRU\n",
    "        # <answer>\n",
    "        self.gru = nn.GRU(input_size=self.input_size, hidden_size=self.hidden_size)\n",
    "        # </answer>\n",
    "\n",
    "        # Linear layer on last hidden state\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # `x` is of size `seq_length` * `batch_size` and `h0` is of size 1\n",
    "        # * `batch_size` * `hidden_size`\n",
    "\n",
    "        # Define first hidden state in not provided\n",
    "        if h0 is None:\n",
    "            # Get batch and define `h0` which is of size 1 *\n",
    "            # `batch_size` * `hidden_size`\n",
    "            # <answer>\n",
    "            batch_size = x.size(1)\n",
    "            h0 = torch.zeros(self.gru.num_layers, batch_size, self.hidden_size).to(device)\n",
    "            # </answer>\n",
    "\n",
    "        # `embedded` is of size `seq_length` * `batch_size` *\n",
    "        # `embedding_dim`\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Define `output` and `hidden` returned by GRU:\n",
    "        #\n",
    "        # - `output` is of size `seq_length` * `batch_size` * `embedding_dim`\n",
    "        #   and gathers all the hidden states along the sequence.\n",
    "        # - `hidden` is of size 1 * `batch_size` * `embedding_dim` and is the\n",
    "        #   last hidden state.\n",
    "        # <answer>\n",
    "        output, hidden = self.gru(embedded, h0)\n",
    "        # </answer>\n",
    "\n",
    "        # Apply a linear layer on the last hidden state to have a\n",
    "        # score tensor of size 1 * `batch_size` * 1, and return a\n",
    "        # tensor of size `batch_size`.\n",
    "        # <answer>\n",
    "        return self.linear(hidden).squeeze()\n",
    "        # </answer>\n",
    "\n",
    "\n",
    "rnn = RNN(hidden_size=100, vocab_vectors=vocab_vectors)\n",
    "print(sum(torch.numel(e) for e in rnn.parameters() if e.requires_grad))\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, rnn.parameters()), lr=0.001)\n",
    "train(rnn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f30551",
   "metadata": {},
   "source": [
    "## CNN based text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "796842e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:58:03.454741800Z",
     "start_time": "2024-01-07T22:58:03.450733200Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_vectors, freeze=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
    "        self.embedding_dim = self.embedding.embedding_dim\n",
    "\n",
    "        self.conv_0 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(3, self.embedding_dim)\n",
    "        )\n",
    "        self.conv_1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(4, self.embedding_dim)\n",
    "        )\n",
    "        self.conv_2 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(5, self.embedding_dim)\n",
    "        )\n",
    "        self.linear = nn.Linear(3 * 100, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input `x` is of size `seq_length` * `batch_size`\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # The tensor `embedded` is of size `seq_length` * `batch_size` *\n",
    "        # `embedding_dim` and should be of size `batch_size` *\n",
    "        # (`n_channels`=1) * `seq_length` * `embedding_dim` for the\n",
    "        # convolutional layers. You can use `transpose` and `unsqueeze` to make\n",
    "        # the transformation.\n",
    "        # <answer>\n",
    "        embedded = embedded.transpose(0, 1).unsqueeze(1)\n",
    "        # </answer>\n",
    "\n",
    "        # Tensor `embedded` is now of size `batch_size` * 1 *\n",
    "        # `seq_length` * `embedding_dim` before convolution and should\n",
    "        # be of size `batch_size` * (`out_channels` = 100) *\n",
    "        # (`seq_length` - `kernel_size[0]` + 1) after convolution and\n",
    "        # squeezing.\n",
    "        # Implement the convolution layer\n",
    "        # <answer>\n",
    "        conved_0 = self.conv_0(embedded).squeeze(3)\n",
    "        conved_1 = self.conv_1(embedded).squeeze(3)\n",
    "        conved_2 = self.conv_2(embedded).squeeze(3)\n",
    "        # </answer>\n",
    "\n",
    "        # Non-linearity step, we use ReLU activation\n",
    "        # <answer>\n",
    "        conved_0_relu = F.relu(conved_0)\n",
    "        conved_1_relu = F.relu(conved_1)\n",
    "        conved_2_relu = F.relu(conved_2)\n",
    "        # </answer>\n",
    "\n",
    "        # Max-pooling layer: pooling along whole sequence\n",
    "        # Implement max pooling\n",
    "        # <answer>\n",
    "        seq_len_0 = conved_0_relu.shape[2]\n",
    "        pooled_0 = F.max_pool1d(conved_0_relu, kernel_size=seq_len_0).squeeze(2)\n",
    "\n",
    "        seq_len_1 = conved_1_relu.shape[2]\n",
    "        pooled_1 = F.max_pool1d(conved_1_relu, kernel_size=seq_len_1).squeeze(2)\n",
    "\n",
    "        seq_len_2 = conved_2_relu.shape[2]\n",
    "        pooled_2 = F.max_pool1d(conved_2_relu, kernel_size=seq_len_2).squeeze(2)\n",
    "        # </answer>\n",
    "\n",
    "        # Dropout on concatenated pooled features\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "\n",
    "        # Linear layer\n",
    "        return self.linear(cat).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acf14e32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T23:00:03.242493300Z",
     "start_time": "2024-01-07T22:58:08.765870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 0.001, Train acc: 0.612, Val loss: 0.001, Val acc: 0.710, Epoch time = 7.847s\n",
      "Epoch: 2, Train loss: 0.001, Train acc: 0.742, Val loss: 0.001, Val acc: 0.756, Epoch time = 7.569s\n",
      "Epoch: 3, Train loss: 0.001, Train acc: 0.792, Val loss: 0.001, Val acc: 0.771, Epoch time = 7.501s\n",
      "Epoch: 4, Train loss: 0.001, Train acc: 0.822, Val loss: 0.001, Val acc: 0.779, Epoch time = 7.405s\n",
      "Epoch: 5, Train loss: 0.001, Train acc: 0.855, Val loss: 0.001, Val acc: 0.779, Epoch time = 7.468s\n",
      "Epoch: 6, Train loss: 0.001, Train acc: 0.881, Val loss: 0.001, Val acc: 0.780, Epoch time = 7.359s\n",
      "Epoch: 7, Train loss: 0.000, Train acc: 0.903, Val loss: 0.001, Val acc: 0.779, Epoch time = 7.446s\n",
      "Epoch: 8, Train loss: 0.000, Train acc: 0.923, Val loss: 0.001, Val acc: 0.774, Epoch time = 7.398s\n",
      "Epoch: 9, Train loss: 0.000, Train acc: 0.941, Val loss: 0.001, Val acc: 0.759, Epoch time = 7.372s\n",
      "Epoch: 10, Train loss: 0.000, Train acc: 0.952, Val loss: 0.001, Val acc: 0.763, Epoch time = 7.338s\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(vocab_vectors)\n",
    "optimizer = optim.Adam(cnn.parameters())\n",
    "train(cnn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfd609",
   "metadata": {},
   "source": [
    "## Test function"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
