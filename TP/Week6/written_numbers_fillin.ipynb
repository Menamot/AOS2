{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2fa135",
   "metadata": {},
   "source": [
    "# The transformer architecture\n",
    "\n",
    "## Needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "from timeit import default_timer as timer\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import Transformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2803ba2dab74858b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39b3a44922fc1ba1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from written_numbers_dataset import NumberDataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a549e4271a2bd986"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vocabulary\n",
    "\n",
    "We first build a vocabulary out of a list of iterators on tokens.\n",
    "Here the vocabulary is already known. To have a vocabulary object,\n",
    "we still use `build_vocab_from_iterator` with `[VOCAB]`.\n",
    "\n",
    "We will also need four different special tokens:\n",
    "\n",
    "- A token for unknown words\n",
    "- A padding token\n",
    "- A token indicating the beginning of a sequence\n",
    "- A token indicating the end of a sequence\n",
    "\n",
    "First we choose a dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14ff488102e959a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a training set and a test set for a dataset.\n",
    "# Number of sequences generated for the training set\n",
    "train_set = ...\n",
    "test_set = ..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "297d7d7c2dcc5276"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "vocab_src = build_vocab_from_iterator([train_set.vocab_src], specials=special_tokens)\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = vocab_src.lookup_indices(special_tokens)\n",
    "vocab_tgt = build_vocab_from_iterator([train_set.vocab_tgt], specials=special_tokens)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bdeda50655cd128"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can test the `vocab` object by giving it a list of tokens."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86f57dea906d4b58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# vocab([<tokens>,...])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47cfcd8d5b62e913"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collate function\n",
    "\n",
    "The collate function is needed to convert a list of samples from their raw\n",
    "form to a Tensor that a Pytorch model can consume. There are two different\n",
    "tasks:\n",
    "\n",
    "- numericalizing the sequence: changing each token in its index in the\n",
    "  vocabulary using the `vocab` object defined earlier\n",
    "- pad sequence so that they have the same length, see [here][pad]\n",
    "\n",
    "[pad]: https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a164addec38a6461"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def collate_fn(batch: List):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "\n",
    "        # Numericalize list of tokens using `vocab`.\n",
    "        #\n",
    "        # - Don't forget to add beginning of sequence and end of sequence tokens\n",
    "        #   before numericalizing.\n",
    "        #\n",
    "        # - Use `torch.LongTensor` instead of `torch.Tensor` because the next\n",
    "        #   step is an embedding that needs integers for its lookup table.\n",
    "        src_tensor = ...\n",
    "        tgt_tensor = ...\n",
    "\n",
    "        # Append numericalized sequence to `src_batch` and `tgt_batch`\n",
    "        src_batch.append(src_tensor)\n",
    "        tgt_batch.append(tgt_tensor)\n",
    "\n",
    "    # Turn `src_batch` and `tgt_batch` that are lists of 1-dimensional\n",
    "    # tensors of varying sizes into tensors with same size with\n",
    "    # padding. Use `pad_sequence` with padding value to do so.\n",
    "    #\n",
    "    # Important notice: by default resulting tensors are of size\n",
    "    # `max_seq_length` * `batch_size`; the mini-batch size is on the\n",
    "    # *second dimension*.\n",
    "    src_batch = ...\n",
    "    tgt_batch = ...\n",
    "\n",
    "    return src_batch, tgt_batch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29d236f2c2bbadcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc33fcb15e92b099"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Size of source and target vocabulary\n",
    "SRC_VOCAB_SIZE = len(vocab_src)\n",
    "TGT_VOCAB_SIZE = len(vocab_tgt)\n",
    "\n",
    "# Number of epochs\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Size of embeddings\n",
    "EMB_SIZE = 128\n",
    "\n",
    "# Number of heads for the multihead attention\n",
    "NHEAD = 1\n",
    "\n",
    "# Size of hidden layer of FFN\n",
    "FFN_HID_DIM = 16\n",
    "\n",
    "# Size of mini-batches\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# Number of stacked encoder modules\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "\n",
    "# Number of stacked decoder modules\n",
    "NUM_DECODER_LAYERS = 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6a7b793c540f98f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Positional encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90e104267d66c292"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float = 0.1, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Define Tk/2pi for even k between 0 and `emb_size`. Use\n",
    "        # `torch.arange`.\n",
    "        Tk_over_2pi = ...\n",
    "\n",
    "        # Define `t = 0, 1,..., maxlen-1`. Use `torch.arange`.\n",
    "        t = ...\n",
    "\n",
    "        # Outer product between `t` and `1/Tk_over_2pi` to have a\n",
    "        # matrix of size `maxlen` * `emb_size // 2`. Use\n",
    "        # `torch.outer`.\n",
    "        outer = ...\n",
    "\n",
    "        pos_embedding = torch.empty((maxlen, emb_size))\n",
    "\n",
    "        # Fill `pos_embedding` with either sine or cosine of `outer`.\n",
    "        pos_embedding[:, 0::2] = ...\n",
    "        pos_embedding[:, 1::2] = ...\n",
    "\n",
    "        # Add fake mini-batch dimension to be able to use broadcasting\n",
    "        # in `forward` method.\n",
    "        pos_embedding = pos_embedding.unsqueeze(1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Save `pos_embedding` when serializing the model even if it is not a\n",
    "        # set of parameters\n",
    "        self.register_buffer(\"pos_embedding\", pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        # `token_embedding` is of size `seq_length` * `batch_size` *\n",
    "        # `embedding_size`. Use broadcasting to add the positional embedding\n",
    "        # that is of size `seq_length` * 1 * `embedding_size`.\n",
    "        seq_length = ...\n",
    "        positional_encoding = ...\n",
    "\n",
    "        return self.dropout(positional_encoding)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90ff65bab68622c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb1f670860083224"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_encoder_layers: int,\n",
    "            num_decoder_layers: int,\n",
    "            emb_size: int,\n",
    "            nhead: int,\n",
    "            src_vocab_size: int,\n",
    "            tgt_vocab_size: int,\n",
    "            dim_feedforward: int = 512,\n",
    "            dropout: float = 0.1,\n",
    "    ):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        # Linear layer to compute a score for all tokens from output\n",
    "        # of transformer\n",
    "        self.generator = \n",
    "\n",
    "        # Embedding for source vocabulary\n",
    "        self.src_tok_emb = ...\n",
    "\n",
    "        # Embedding for target vocabulary\n",
    "        self.tgt_tok_emb = ...\n",
    "\n",
    "        # Positional encoding layer\n",
    "        self.positional_encoding = ...\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            src: Tensor,\n",
    "            trg: Tensor,\n",
    "            src_mask: Tensor,\n",
    "            tgt_mask: Tensor,\n",
    "            src_padding_mask: Tensor,\n",
    "            tgt_padding_mask: Tensor,\n",
    "            memory_key_padding_mask: Tensor,\n",
    "    ):\n",
    "        # Embed `src` and `trg` tensors and add positional embedding.\n",
    "        src_emb = ...\n",
    "        tgt_emb = ...\n",
    "\n",
    "        outs = self.transformer(\n",
    "            src_emb,\n",
    "            tgt_emb,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            None,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            memory_key_padding_mask,\n",
    "        )\n",
    "\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        # Use the encoder part of the transformer to encode `src`.\n",
    "        return ...\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        # Use the decoder par of the transformer to decode `tgt`\n",
    "        return ...\n",
    "\n",
    "    def encode_and_attention(self, src: Tensor, src_mask: Tensor):\n",
    "        \"\"\"Used at test-time only to retrieve attention matrix.\"\"\"\n",
    "\n",
    "        src_pos = self.positional_encoding(self.src_tok_emb(src))\n",
    "        self_attn = self.transformer.encoder.layers[-1].self_attn\n",
    "        att = self_attn(src_pos, src_pos, src_pos, attn_mask=src_mask)[1]\n",
    "        return self.encode(src, src_mask), att\n",
    "\n",
    "    def decode_and_attention(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        \"\"\"Used at test-time only to retrieve attention matrix.\"\"\"\n",
    "\n",
    "        # Use first decoder layer\n",
    "        decoder = self.transformer.decoder.layers[0]\n",
    "\n",
    "        x = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        x = decoder.norm1(x + decoder._sa_block(x, tgt_mask, None))\n",
    "        att = decoder.multihead_attn(x, memory, memory, need_weights=True)[1]\n",
    "\n",
    "        return self.transformer.decoder(x, memory, tgt_mask), att"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c763eda7e9b73f7d"
  },
  {
   "cell_type": "markdown",
   "id": "820670bf",
   "metadata": {},
   "source": [
    "## Mask function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src: Tensor, tgt: Tensor):\n",
    "    # Lengths of source and target sequences\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    # Attention mask for the source. As we have no reason to mask input\n",
    "    # tokens, we use a mask full of False. You can use `torch.full`.\n",
    "    src_mask = ...\n",
    "\n",
    "    # Attention mask for the target. To prevent a token from receiving\n",
    "    # attention from future ones, we use a mask as defined in the lecture\n",
    "    # (matrix `M`). You can use `torch.triu` and `torch.full` or directly\n",
    "    # use the static function `generate_square_subsequent_mask` from the\n",
    "    # `Transformer` class.\n",
    "    tgt_mask = ...\n",
    "\n",
    "    # Boolean masks identifying tokens that have been padded with\n",
    "    # `PAD_IDX`. Use `src` and `tgt` to create them. Don't forget to\n",
    "    # ajust the size since both `src` and `tgt` are of size\n",
    "    # `batch_size` * `seq_len` and the transformer object needs masks\n",
    "    # of size `seq_len` * `batch_size`.\n",
    "    src_padding_mask = ...\n",
    "    tgt_padding_mask = ...\n",
    "\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a55d166",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a64974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, dataset: Dataset, optimizer: Optimizer):\n",
    "    # Training mode\n",
    "    model.train()\n",
    "\n",
    "    # Set loss function to use. Don't forget to tell the loss function to\n",
    "    # ignore entries that are padded.\n",
    "    loss_fn = ...\n",
    "\n",
    "    # Turn `dataset` into an iterable on mini-batches using `DataLoader`.\n",
    "    train_dataloader = ...\n",
    "\n",
    "    losses = 0\n",
    "    for src, tgt in train_dataloader:\n",
    "        # Select all but the last element of each sequence in `tgt`\n",
    "        tgt_input = ...\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
    "            src, tgt_input\n",
    "        )\n",
    "\n",
    "        scores = model(\n",
    "            src,\n",
    "            tgt_input,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            src_padding_mask,\n",
    "        )\n",
    "\n",
    "        # Resetting gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Select all but the first element of each sequence in `tgt`\n",
    "        tgt_out = ...\n",
    "\n",
    "        # Permute dimensions before cross-entropy loss:\n",
    "        #\n",
    "        # - `logits` is `seq_length` * `batch_size` * `vocab_size` and should be\n",
    "        #   `batch_size` * `vocab_size` * `seq_length`\n",
    "        # - `tgt_out` is `seq_length` * `batch_size` and should be\n",
    "        #   `batch_size` * `seq_length`\n",
    "        loss = ...\n",
    "\n",
    "        # Back-propagation through loss function\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient descent update\n",
    "        optimizer.step()\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869d28e",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, val_dataset: Dataset):\n",
    "    model.eval()\n",
    "\n",
    "    # Set loss function to use. Don't forget to tell the loss function to\n",
    "    # ignore entries that are padded.\n",
    "    loss_fn = ...\n",
    "\n",
    "    # Turn dataset into an iterable on batches\n",
    "    val_dataloader = ...\n",
    "\n",
    "    losses = 0\n",
    "    for src, tgt in val_dataloader:\n",
    "        # Select all but the last element of each sequence in `tgt`\n",
    "        tgt_input = ...\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
    "            src, tgt_input\n",
    "        )\n",
    "\n",
    "        logits = model(\n",
    "            src,\n",
    "            tgt_input,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            src_padding_mask,\n",
    "        )\n",
    "\n",
    "        # Select all but the first element of each sequence in `tgt`\n",
    "        tgt_out = ...\n",
    "\n",
    "        # Permute dimensions for cross-entropy loss:\n",
    "        #\n",
    "        # - `logits` is `seq_length` * `batch_size` * `vocab_size` and should be\n",
    "        #   `batch_size` * `vocab_size` * `seq_length`\n",
    "        # - `tgt_out` is `seq_length` * `batch_size` and should be\n",
    "        #   `batch_size` * `seq_length`\n",
    "        loss = ...\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff3be0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Learning loop\n",
    "\n",
    "transformer = Seq2SeqTransformer(\n",
    "    NUM_ENCODER_LAYERS,\n",
    "    NUM_DECODER_LAYERS,\n",
    "    EMB_SIZE,\n",
    "    NHEAD,\n",
    "    SRC_VOCAB_SIZE,\n",
    "    TGT_VOCAB_SIZE,\n",
    "    FFN_HID_DIM,\n",
    ")\n",
    "\n",
    "optimizer = Adam(transformer.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, train_set, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer, test_set)\n",
    "    print(\n",
    "        (\n",
    "            f\"Epoch: {epoch}, Train loss: {train_loss:.5f}, Val loss: {val_loss:.5f}, \"\n",
    "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0d190",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Helpers functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853eafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, start_symbol_idx):\n",
    "    \"\"\"Autoregressive decoding of `src` starting with `start_symbol_idx`.\"\"\"\n",
    "\n",
    "    memory, att = model.encode_and_attention(src, src_mask)\n",
    "    ys = torch.LongTensor([[start_symbol_idx]])\n",
    "    maxlen = 100\n",
    "\n",
    "    for i in range(maxlen):\n",
    "        tgt_mask = Transformer.generate_square_subsequent_mask(ys.size(0))\n",
    "\n",
    "        # Decode `ys`. `out` is of size `curr_len` * 1 * `vocab_size`\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "\n",
    "        # Select encoding of last token\n",
    "        enc = out[-1, 0, :]\n",
    "\n",
    "        # Get a set of scores on vocabulary\n",
    "        dist = model.generator(enc)\n",
    "\n",
    "        # Get index of maximum\n",
    "        idx = torch.argmax(dist).item()\n",
    "\n",
    "        # Add predicted index to `ys`\n",
    "        ys = torch.cat((ys, torch.LongTensor([[idx]])))\n",
    "\n",
    "        if idx == EOS_IDX:\n",
    "            break\n",
    "    return ys, att\n",
    "\n",
    "\n",
    "def translate(model: torch.nn.Module, src_sentence: Iterable):\n",
    "    \"\"\"Translate sequence `src_sentence` with `model`.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Numericalize source\n",
    "    src_tensor = torch.LongTensor(vocab_src([\"<bos>\"] + list(src_sentence) + [\"<eos>\"]))\n",
    "\n",
    "    # Fake a minibatch of size one\n",
    "    src = src_tensor.unsqueeze(-1)\n",
    "\n",
    "    # No mask for source sequence\n",
    "    seq_length = src.size(0)\n",
    "    src_mask = torch.full((seq_length, seq_length), False)\n",
    "\n",
    "    # Translate `src`\n",
    "    tgt_tokens, att = greedy_decode(model, src, src_mask, BOS_IDX)\n",
    "\n",
    "    tgt_tokens = tgt_tokens.flatten().numpy()\n",
    "    att = att.detach().squeeze().numpy()\n",
    "    return \" \".join(vocab_tgt.lookup_tokens(list(tgt_tokens))), att\n",
    "\n",
    "\n",
    "def plot_encoder_attention_matrix(model, src):\n",
    "    \"\"\"Plot heatmap of encoder's attention matrix.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Numericalize source\n",
    "    src_delim = [\"<bos>\"] + list(src) + [\"<eos>\"]\n",
    "    src_tensor = torch.LongTensor(vocab_src(src_delim))\n",
    "\n",
    "    # Fake a minibatch of size one\n",
    "    src = src_tensor.unsqueeze(-1)\n",
    "\n",
    "    # No mask for source sequence\n",
    "    seq_length = src.size(0)\n",
    "    src_mask = torch.full((seq_length, seq_length), False)\n",
    "\n",
    "    # Translate `src`\n",
    "    memory, att = model.encode_and_attention(src, src_mask)\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        att.detach().squeeze().numpy(),\n",
    "        xticklabels=src_delim,\n",
    "        yticklabels=src_delim,\n",
    "    )\n",
    "    ax.set(xlabel='Key', ylabel='Query')\n",
    "\n",
    "    ax.tick_params(\n",
    "        axis=\"both\",\n",
    "        which=\"major\",\n",
    "        labelsize=10,\n",
    "        labelbottom=False,\n",
    "        bottom=False,\n",
    "        top=False,\n",
    "        labeltop=True,\n",
    "    )\n",
    "\n",
    "def plot_decoder_attention_matrix(model, src, tgt):\n",
    "    \"\"\"Plot heatmap of decoder's cross-attention matrix.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Numericalize source and target\n",
    "    src_delim = [\"<bos>\"] + list(src) + [\"<eos>\"]\n",
    "    src_tensor = torch.LongTensor(vocab_src(src_delim))\n",
    "    tgt_delim = [\"<bos>\"] + list(tgt) + [\"<eos>\"]\n",
    "    tgt_tensor = torch.LongTensor(vocab_tgt(tgt_delim))\n",
    "\n",
    "    # Fake a minibatch of size one\n",
    "    src = src_tensor.unsqueeze(-1)\n",
    "    tgt = tgt_tensor.unsqueeze(-1)\n",
    "\n",
    "    # No mask for source sequence and triangular mask to target\n",
    "    seq_length = src.size(0)\n",
    "    src_mask = torch.full((seq_length, seq_length), False)\n",
    "    tgt_mask = Transformer.generate_square_subsequent_mask(tgt.size(0))\n",
    "\n",
    "    # Encode `src`\n",
    "    memory = model.encode(src, src_mask)\n",
    "\n",
    "    # Retrieve cross-attention matrix\n",
    "    _, att = model.decode_and_attention(tgt, memory, tgt_mask)\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        att.detach().squeeze().numpy(),\n",
    "        xticklabels=src_delim,\n",
    "        yticklabels=tgt_delim,\n",
    "    )\n",
    "    ax.set(xlabel='Key', ylabel='Query')\n",
    "\n",
    "    ax.tick_params(\n",
    "        axis=\"both\",\n",
    "        which=\"major\",\n",
    "        labelsize=10,\n",
    "        labelbottom=False,\n",
    "        bottom=False,\n",
    "        top=False,\n",
    "        labeltop=True,\n",
    "    )\n",
    "\n",
    "\n",
    "src, tgt = test_set[0]\n",
    "pred, att = translate(transformer, src)\n",
    "\n",
    "plot_encoder_attention_matrix(transformer, src)\n",
    "plt.show()\n",
    "\n",
    "plot_decoder_attention_matrix(transformer, src, tgt)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
