{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f616d1",
   "metadata": {},
   "source": [
    "# Image Style Transfer Using Convolutional Neural Networks\n",
    "\n",
    "This notebook implements the algorithm found in [(Gatys\n",
    "2016)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as utils\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "imsize = 128\n",
    "loader = transforms.Compose([transforms.Resize(imsize), transforms.ToTensor()])\n",
    "\n",
    "# Load `content_img` as a torch tensor of size 3 * `imsize` * `imsize`\n",
    "image = Image.open(\"./data/images/dancing.jpg\")\n",
    "content_img = loader(image)\n",
    "\n",
    "# Load `style_img` as a torch tensor of size 3 * `imsize` * `imsize`\n",
    "image = Image.open(\"./data/images/mondrian.jpg\")\n",
    "style_img = loader(image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T17:17:39.921981500Z",
     "start_time": "2024-01-09T17:17:39.845468300Z"
    }
   },
   "id": "f5b3b6e49e3bd337"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Feature extraction with VGG19\n",
    "The next cell is a CNN based on VGG19 which extracts convolutional\n",
    "features specified by `modules_indexes`. It is used to compute the\n",
    "features of the content and style image. It is also used to\n",
    "reconstruct the target image by backpropagation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d50f932ecec4bc80"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class VGG19Features(nn.Module):\n",
    "    def __init__(self, modules_indexes):\n",
    "        super(VGG19Features, self).__init__()\n",
    "\n",
    "        # VGG19 pretrained model in evaluation mode\n",
    "        self.vgg19 = models.vgg19(weights=models.VGG19_Weights.DEFAULT).eval()\n",
    "\n",
    "        # Indexes of layers to remember\n",
    "        self.modules_indexes = modules_indexes\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Define a hardcoded `mean` and `std` of size 3 * 1 * 1\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "\n",
    "        # First center and normalize `input` with `mean` and `std`\n",
    "        input_norm = (input - mean)/std\n",
    "\n",
    "        # Add a fake mini-batch dimension to `input_norm`\n",
    "        input_norm = input_norm.unsqueeze(0)\n",
    "\n",
    "        # Install hooks on specified modules to save their features\n",
    "        features = []\n",
    "        handles = []\n",
    "        for module_index in self.modules_indexes:\n",
    "\n",
    "            def hook(module, input, output):\n",
    "                # `output` is of size (`batchsize` = 1) * `n_filters`\n",
    "                # * `imsize` * `imsize`\n",
    "                features.append(output)\n",
    "\n",
    "            handle = self.vgg19.features[module_index].register_forward_hook(hook)\n",
    "            handles.append(handle)\n",
    "\n",
    "        # Forward propagate `input_norm`. This will trigger the hooks\n",
    "        # set up above and populate `features`\n",
    "        self.vgg19(input_norm)\n",
    "\n",
    "        # Remove hooks\n",
    "        [handle.remove() for handle in handles]\n",
    "\n",
    "        # The output of our custom VGG19Features neural network is a\n",
    "        # list of features of `input`\n",
    "        return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T17:18:43.397498Z",
     "start_time": "2024-01-09T17:18:43.389493200Z"
    }
   },
   "id": "fec2ae5506173008"
  },
  {
   "cell_type": "markdown",
   "id": "99706fb9",
   "metadata": {},
   "source": [
    "\n",
    "The next cell defines the convolutional layers we will use to\n",
    "capture the style and the content. Look at the paper to see what are\n",
    "those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a4f27f",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2024-01-09T17:18:45.989612300Z",
     "start_time": "2024-01-09T17:18:44.416742100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Indexes of interesting features to extract\n",
    "\n",
    "# Define `modules_indexes`\n",
    "modules_indexes = [0,4]\n",
    "\n",
    "vgg19 = VGG19Features(modules_indexes)\n",
    "content_features = [f.detach() for f in vgg19.forward(content_img)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a7467",
   "metadata": {},
   "source": [
    "\n",
    "## Style features as gram matrix of convolutional features\n",
    "\n",
    "The next cell computes the gram matrix of `input`. We first need to\n",
    "reshape `input` before computing the gram matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input):\n",
    "    batchsize, n_filters, width, height = input.size()\n",
    "\n",
    "    # Reshape `input` into `n_filters` * `n_pixels`\n",
    "    features = input.view(n_filters,-1)\n",
    "\n",
    "    # Compute the inner products between filters in `G`\n",
    "    G = torch.mm(features,features.t())\n",
    "\n",
    "    # We `normalize` the values of the gram matrix by dividing by the\n",
    "    # number of element in each feature maps.\n",
    "    return G.div(n_filters * width * height)\n",
    "\n",
    "\n",
    "style_gram_features = [gram_matrix(f.detach()) for f in vgg19.forward(style_img)]\n",
    "\n",
    "target = content_img.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6de515",
   "metadata": {},
   "source": [
    "\n",
    "## Optimizer\n",
    "\n",
    "Look at the paper to see what is the algorithm they are using.\n",
    "Remember that we are optimizing on a target image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define `optimizer` to use L-BFGS algorithm to do gradient descent on\n",
    "# `target`\n",
    "optimizer = torch.optim.LBFGS([target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6106509",
   "metadata": {},
   "source": [
    "\n",
    "## The algorithm\n",
    "\n",
    "From the paper, there are two different losses. The style loss and the\n",
    "content loss.\n",
    "\n",
    "Define `style_weight` the trade-off parameter between style and\n",
    "content losses\n",
    "style_weight = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f37d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for step in range(500):\n",
    "    # To keep track of the losses in the closure\n",
    "    losses = {}\n",
    "\n",
    "    # Need to use a closure that computes the loss and gradients to allow the\n",
    "    # optimizer to evaluate repeatedly at different locations\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # First, forward propagate `target` through our VGG19Features neural\n",
    "        # network and store its output as `target_features`\n",
    "        target_features = vgg19(target)\n",
    "\n",
    "        # Define `content_loss` on the first layer only\n",
    "        content_loss = torch.sum((target_features[0] - content_features[0])**2)\n",
    "\n",
    "        style_loss = 0\n",
    "        for target_feature, style_gram_feature in zip(target_features, style_gram_features):\n",
    "            # Compute Gram matrix\n",
    "            target_gram_feature = ...\n",
    "\n",
    "            # Add current loss to `style_loss`\n",
    "            style_loss += ...\n",
    "\n",
    "        # Compute combined loss\n",
    "        loss = ...\n",
    "\n",
    "        # Store the losses\n",
    "        losses[\"loss\"] = loss.item()\n",
    "        losses[\"style_loss\"] = style_loss.item()\n",
    "        losses[\"content_loss\"] = content_loss.item()\n",
    "\n",
    "        # Backward propagation and return loss\n",
    "        ...\n",
    "\n",
    "    # Gradient step : don't forget to pass the closure to the optimizer\n",
    "    ...\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(\"step {}:\".format(step))\n",
    "        print(\n",
    "            \"Style Loss: {:4f} Content Loss: {:4f} Overall: {:4f}\".format(\n",
    "                losses[\"style_loss\"], losses[\"content_loss\"], losses[\"loss\"]\n",
    "            )\n",
    "        )\n",
    "        img = target.clone().squeeze()\n",
    "        img = img.clamp_(0, 1)\n",
    "        utils.save_image(img, \"output-{}.png\".format(step))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
