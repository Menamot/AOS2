{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1fbc5a",
   "metadata": {},
   "source": [
    "# Image Style Transfer Using Convolutional Neural Networks\n",
    "\n",
    "This notebook implements the algorithm found in [(Gatys\n",
    "2016)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as utils\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "imsize = 128\n",
    "loader = transforms.Compose([transforms.Resize(imsize), transforms.ToTensor()])\n",
    "\n",
    "# Load `content_img` as a torch tensor of size 3 * `imsize` * `imsize`\n",
    "image = Image.open(\"./data/images/dancing.jpg\")\n",
    "content_img = loader(image)\n",
    "\n",
    "# Load `style_img` as a torch tensor of size 3 * `imsize` * `imsize`\n",
    "image = Image.open(\"./data/images/mondrian.jpg\")\n",
    "style_img = loader(image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T18:12:08.997086600Z",
     "start_time": "2024-01-07T18:12:04.826607900Z"
    }
   },
   "id": "cd200e63da4b1273"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Feature extraction with VGG19\n",
    "The next cell is a CNN based on VGG19 which extracts convolutional\n",
    "features specified by `modules_indexes`. It is used to compute the\n",
    "features of the content and style image. It is also used to\n",
    "reconstruct the target image by backpropagation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4547bc7f9ca0df8"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class VGG19Features(nn.Module):\n",
    "    def __init__(self, modules_indexes):\n",
    "        super(VGG19Features, self).__init__()\n",
    "\n",
    "        # VGG19 pretrained model in evaluation mode\n",
    "        self.vgg19 = models.vgg19(weights=models.VGG19_Weights.DEFAULT).eval()\n",
    "\n",
    "        # Indexes of layers to remember\n",
    "        self.modules_indexes = modules_indexes\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Define a hardcoded `mean` and `std` of size 3 * 1 * 1\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "\n",
    "        # First center and normalize `input` with `mean` and `std`\n",
    "        # <answer>\n",
    "        input_norm = (input - mean) / std\n",
    "        # </answer>\n",
    "\n",
    "        # Add a fake mini-batch dimension to `input_norm`\n",
    "        # <answer>\n",
    "        input_norm = input_norm.unsqueeze(0)  # \n",
    "        # </answer>\n",
    "\n",
    "        # Install hooks on specified modules to save their features\n",
    "        features = []\n",
    "        handles = []\n",
    "        for module_index in self.modules_indexes:\n",
    "\n",
    "            def hook(module, input, output):\n",
    "                # `output` is of size (`batchsize` = 1) * `n_filters`\n",
    "                # * `imsize` * `imsize`\n",
    "                features.append(output)\n",
    "\n",
    "            handle = self.vgg19.features[module_index].register_forward_hook(hook)\n",
    "            handles.append(handle)\n",
    "\n",
    "        # Forward propagate `input_norm`. This will trigger the hooks\n",
    "        # set up above and populate `features`\n",
    "        self.vgg19(input_norm)\n",
    "\n",
    "        # Remove hooks\n",
    "        [handle.remove() for handle in handles]\n",
    "\n",
    "        # The output of our custom VGG19Features neural network is a\n",
    "        # list of features of `input`\n",
    "        return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T18:12:10.836417500Z",
     "start_time": "2024-01-07T18:12:10.829413300Z"
    }
   },
   "id": "a5552ed1f2889ad7"
  },
  {
   "cell_type": "markdown",
   "id": "b8498af6",
   "metadata": {},
   "source": [
    "\n",
    "The next cell defines the convolutional layers we will use to\n",
    "capture the style and the content. Look at the paper to see what are\n",
    "those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "__main__.VGG19Features"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T18:12:27.207674Z",
     "start_time": "2024-01-07T18:12:27.199832600Z"
    }
   },
   "id": "bcdc3dd4fea65788"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49e979b3",
   "metadata": {
    "lines_to_next_cell": 1,
    "ExecuteTime": {
     "end_time": "2024-01-07T18:17:33.845208500Z",
     "start_time": "2024-01-07T18:17:32.209558400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "127.0"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexes of interesting features to extract\n",
    "\n",
    "# Define `modules_indexes`\n",
    "# <answer>\n",
    "modules_indexes = [0,4]\n",
    "# </answer>\n",
    "\n",
    "vgg19 = VGG19Features(modules_indexes)\n",
    "content_features = [f.detach() for f in vgg19.forward(content_img)]\n",
    "vgg19\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b8f62",
   "metadata": {},
   "source": [
    "\n",
    "## Style features as gram matrix of convolutional features\n",
    "\n",
    "The next cell computes the gram matrix of `input`. We first need to\n",
    "reshape `input` before computing the gram matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bca4eaf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T19:00:05.019786900Z",
     "start_time": "2023-12-02T19:00:04.940490600Z"
    }
   },
   "outputs": [],
   "source": [
    "def gram_matrix(input):\n",
    "    batchsize, n_filters, width, height = input.size()\n",
    "\n",
    "    # Reshape `input` into `n_filters` * `n_pixels`\n",
    "    # <answer>\n",
    "    features = input.view(n_filters, width*height)\n",
    "    # </answer>\n",
    "\n",
    "    # Compute the inner products between filters in `G`\n",
    "    # <answer>\n",
    "    G = torch.mm(features, features.t())\n",
    "    # </answer>\n",
    "\n",
    "    # We `normalize` the values of the gram matrix by dividing by the\n",
    "    # number of element in each feature maps.\n",
    "    return G.div(n_filters * width * height)\n",
    "\n",
    "\n",
    "style_gram_features = [gram_matrix(f.detach()) for f in vgg19.forward(style_img)]\n",
    "\n",
    "target = content_img.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee36843",
   "metadata": {},
   "source": [
    "\n",
    "## Optimizer\n",
    "\n",
    "Look at the paper to see what is the algorithm they are using.\n",
    "Remember that we are optimizing on a target image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f55765b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T19:00:05.020785400Z",
     "start_time": "2023-12-02T19:00:05.003267400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define `optimizer` to use L-BFGS algorithm to do gradient descent on\n",
    "# `target`\n",
    "# <answer>\n",
    "optimizer = optim.LBFGS([target])\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aac5e4",
   "metadata": {},
   "source": [
    "\n",
    "## The algorithm\n",
    "\n",
    "From the paper, there are two different losses. The style loss and the\n",
    "content loss.\n",
    "\n",
    "Define `style_weight` the trade-off parameter between style and\n",
    "content losses\n",
    "<answer>\n",
    "style_weight = 10**6\n",
    "</answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b4f4161",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T19:04:22.662678500Z",
     "start_time": "2023-12-02T19:00:05.007786100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0:\n",
      "Style Loss: 30.215357 Content Loss: 13290.753906 Overall: 43506.109375\n",
      "step 10:\n",
      "Style Loss: 21.487640 Content Loss: 15102.208008 Overall: 36589.847656\n",
      "step 20:\n",
      "Style Loss: 21.010372 Content Loss: 15216.641602 Overall: 36227.015625\n",
      "step 30:\n",
      "Style Loss: 20.858278 Content Loss: 15236.527344 Overall: 36094.804688\n",
      "step 40:\n",
      "Style Loss: 20.769674 Content Loss: 15247.013672 Overall: 36016.687500\n",
      "step 50:\n",
      "Style Loss: 20.710712 Content Loss: 15253.328125 Overall: 35964.039062\n",
      "step 60:\n",
      "Style Loss: 20.663671 Content Loss: 15264.585938 Overall: 35928.257812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 52\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;66;03m# </answer>\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# Gradient step : don't forget to pass the closure to the optimizer\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# <answer>\u001B[39;00m\n\u001B[1;32m---> 52\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;66;03m# </answer>\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m step \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mD:\\Programming\\Conda_envs\\ML\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    368\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    369\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    370\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    371\u001B[0m             )\n\u001B[1;32m--> 373\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    374\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    376\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Programming\\Conda_envs\\ML\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Programming\\Conda_envs\\ML\\Lib\\site-packages\\torch\\optim\\lbfgs.py:382\u001B[0m, in \u001B[0;36mLBFGS.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    380\u001B[0m q \u001B[38;5;241m=\u001B[39m flat_grad\u001B[38;5;241m.\u001B[39mneg()\n\u001B[0;32m    381\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_old \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m--> 382\u001B[0m     al[i] \u001B[38;5;241m=\u001B[39m \u001B[43mold_stps\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m ro[i]\n\u001B[0;32m    383\u001B[0m     q\u001B[38;5;241m.\u001B[39madd_(old_dirs[i], alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mal[i])\n\u001B[0;32m    385\u001B[0m \u001B[38;5;66;03m# multiply by initial Hessian\u001B[39;00m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;66;03m# r/d is the final direction\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for step in range(500):\n",
    "    # To keep track of the losses in the closure\n",
    "    losses = {}\n",
    "\n",
    "    # Need to use a closure that computes the loss and gradients to allow the\n",
    "    # optimizer to evaluate repeatedly at different locations\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # First, forward propagate `target` through our VGG19Features neural\n",
    "        # network and store its output as `target_features`\n",
    "        # <answer>\n",
    "        target_features = vgg19(target)\n",
    "        # </answer>\n",
    "\n",
    "        # Define `content_loss` on the first layer only\n",
    "        # <answer>\n",
    "        content_loss = torch.sum((target_features[0] - content_features[0])**2)\n",
    "        # </answer>\n",
    "\n",
    "        style_loss = 0\n",
    "        for target_feature, style_gram_feature in zip(target_features, style_gram_features):\n",
    "            # Compute Gram matrix\n",
    "            # <answer>\n",
    "            target_gram_feature = gram_matrix(target_feature)\n",
    "            # </answer>\n",
    "\n",
    "            # Add current loss to `style_loss`\n",
    "            # <answer>\n",
    "            style_loss += torch.sum((target_gram_feature - style_gram_feature)**2)\n",
    "            # </answer>\n",
    "\n",
    "        # Compute combined loss\n",
    "        # <answer>\n",
    "        style_weight=1e3\n",
    "        loss = content_loss + style_weight * style_loss\n",
    "        # </answer>\n",
    "\n",
    "        # Store the losses\n",
    "        losses[\"loss\"] = loss.item()\n",
    "        losses[\"style_loss\"] = style_loss.item()\n",
    "        losses[\"content_loss\"] = content_loss.item()\n",
    "\n",
    "        # Backward propagation and return loss\n",
    "        # <answer>\n",
    "        loss.backward()\n",
    "        return loss\n",
    "        # </answer>\n",
    "\n",
    "    # Gradient step : don't forget to pass the closure to the optimizer\n",
    "    # <answer>\n",
    "    optimizer.step(closure)\n",
    "    # </answer>\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(\"step {}:\".format(step))\n",
    "        print(\n",
    "            \"Style Loss: {:4f} Content Loss: {:4f} Overall: {:4f}\".format(\n",
    "                losses[\"style_loss\"], losses[\"content_loss\"], losses[\"loss\"]\n",
    "            )\n",
    "        )\n",
    "        img = target.clone().squeeze()\n",
    "        img = img.clamp_(0, 1)\n",
    "        utils.save_image(img, \"output-{}.png\".format(step))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
